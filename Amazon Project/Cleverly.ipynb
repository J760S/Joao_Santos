{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ast\n",
    "from textblob import TextBlob\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"Amazon_Reviews_Clothing_Shoes_and_Jewelry_5_sample.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00008ID1O</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>5</td>\n",
       "      <td>I was waiting to have a bad odor from these pa...</td>\n",
       "      <td>09 10, 2012</td>\n",
       "      <td>ARZLW6MFV58E9</td>\n",
       "      <td>Denise Elkin-andrews \"dee\"</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>1347235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00008ID1O</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>I have worn these for years, but was concerned...</td>\n",
       "      <td>01 5, 2014</td>\n",
       "      <td>A454H175JAT32</td>\n",
       "      <td>Emily</td>\n",
       "      <td>some odor, but not really a concern</td>\n",
       "      <td>1388880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00008ID1O</td>\n",
       "      <td>[7, 7]</td>\n",
       "      <td>1</td>\n",
       "      <td>Horrible smell gave me an asthmatic attack whe...</td>\n",
       "      <td>07 19, 2013</td>\n",
       "      <td>A1X5NM3HXQANFL</td>\n",
       "      <td>Jasminenirvana</td>\n",
       "      <td>Terrible Chemical Smell</td>\n",
       "      <td>1374192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00008ID1O</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>3</td>\n",
       "      <td>These are really verging on &amp;#34;granny pantie...</td>\n",
       "      <td>10 15, 2013</td>\n",
       "      <td>AVKF05BAX2Z3I</td>\n",
       "      <td>K. H. Noland</td>\n",
       "      <td>If these are bikini then I hate to see briefs!</td>\n",
       "      <td>1381795200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00008ID1O</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>5</td>\n",
       "      <td>Have been wearing these for years, but was hes...</td>\n",
       "      <td>03 21, 2013</td>\n",
       "      <td>AQJWVL7YBSMOL</td>\n",
       "      <td>Mail Debaser</td>\n",
       "      <td>Fine for Me</td>\n",
       "      <td>1363824000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  B00008ID1O  [3, 4]        5   \n",
       "1  B00008ID1O  [1, 1]        4   \n",
       "2  B00008ID1O  [7, 7]        1   \n",
       "3  B00008ID1O  [0, 2]        3   \n",
       "4  B00008ID1O  [3, 3]        5   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  I was waiting to have a bad odor from these pa...  09 10, 2012   \n",
       "1  I have worn these for years, but was concerned...   01 5, 2014   \n",
       "2  Horrible smell gave me an asthmatic attack whe...  07 19, 2013   \n",
       "3  These are really verging on &#34;granny pantie...  10 15, 2013   \n",
       "4  Have been wearing these for years, but was hes...  03 21, 2013   \n",
       "\n",
       "       reviewerID                reviewerName  \\\n",
       "0   ARZLW6MFV58E9  Denise Elkin-andrews \"dee\"   \n",
       "1   A454H175JAT32                       Emily   \n",
       "2  A1X5NM3HXQANFL              Jasminenirvana   \n",
       "3   AVKF05BAX2Z3I                K. H. Noland   \n",
       "4   AQJWVL7YBSMOL                Mail Debaser   \n",
       "\n",
       "                                          summary  unixReviewTime  \n",
       "0                                       Excellent      1347235200  \n",
       "1             some odor, but not really a concern      1388880000  \n",
       "2                         Terrible Chemical Smell      1374192000  \n",
       "3  If these are bikini then I hate to see briefs!      1381795200  \n",
       "4                                     Fine for Me      1363824000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><li>Is there a correlation between the rating of the product and the helpfulness of the review?</li></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the helpful column into nº of Yes votes and total number of votes\n",
    "\n",
    "df['helpful_column_list'] = df[\"helpful\"].apply(lambda x: ast.literal_eval(str(x)))\n",
    "df[['Nº of YES votes','Nº of Total votes']] = pd.DataFrame(df[\"helpful_column_list\"].values.tolist(), index= df.index)\n",
    "\n",
    "#Convert helpful column to a probability: [3,4] e.g. represents a 3/4 probability of choosing \"Yes\" to the review question. \n",
    "\n",
    "df[\"Helpfulness ratio\"] = df[\"Nº of YES votes\"] / df[\"Nº of Total votes\"]\n",
    "\n",
    "#Substitute NaN values by 0, since our methodology gave 0/0 providing the obvious NaN:\n",
    "\n",
    "df[\"Helpfulness ratio\"] = df[\"Helpfulness ratio\"].fillna(0)\n",
    "\n",
    "#Cleaning Dataframe\n",
    "\n",
    "corr_rating_helpfulness =  df[[\"Helpfulness ratio\", \"overall\"]]\n",
    "\n",
    "#Correlation between Helpfulness ratio and Overall rating\n",
    "\n",
    "corr_rating_helpfulness['Helpfulness ratio'].corr(corr_rating_helpfulness['overall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><li>Suggested Answer:</li></h4>\n",
    "When the value of the correlation is close to zero, generally between -0.1 and +0.1, the variables are said to have no linear relationship or a very weak linear relationship. Since the relationship between \"Helpfulness ratio\" and \"overall\" is approximately -0.03, it is possible to state that approximately 0.09% of variation in Helpfulness of the review can be explained by the rating of the product, meaning that around 99.9% is left to explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<h4><li>Who are the most helpful reviewers?</li></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert ReviwerID column to list\n",
    "\n",
    "reviewID_list = df[\"reviewerID\"].tolist()\n",
    "uniqueID = list(sorted(set(reviewID_list)))\n",
    "\n",
    "#Query all the reviwersID: Helpfulness ratio higher or equal than 0.9 and Nº of total votes higher than 8.\n",
    "\n",
    "ID_count = {}\n",
    "\n",
    "for i in uniqueID:\n",
    "    df_ID = df[df[\"reviewerID\"] == i]\n",
    "    df_ID = df_ID[df_ID[\"Helpfulness ratio\"] >= 0.9]\n",
    "    df_ID = df_ID[df_ID[\"Nº of Total votes\"] > 8]\n",
    "    good_reviews = df_ID.shape[0]\n",
    "    ID_count[i] = good_reviews\n",
    "    \n",
    "#Most helpful reviewers: the presented ID's are individuals that had more than 1 sucessful review.    \n",
    "    \n",
    "most_helpful = {k: v for k, v in ID_count.items() if v > 1}\n",
    "\n",
    "#Passing the most helpful reviewers ID into a list, to represent later the proper reviewerName:\n",
    "\n",
    "reviewerID = list(most_helpful)  \n",
    "\n",
    "for reviewer in reviewerID:\n",
    "    df1 = df[df[\"reviewerID\"] == reviewer]\n",
    "    name = df1.iloc[1][\"reviewerName\"]\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><li>Suggested Answer:</li></h4>\n",
    "Since the question is a bit too abstract, I decided to implement my own logic into solving this problem. I decided to query all the ReviewersID's into only the individuals that have more than 8 total votes in a specific review and that had an helpfulness ratio higher than 90%. From this, I was able to verify which reviewers had the most occurencies that satisfied this type of conditions, which were: Sakura67456, Matthew G. Sherwin, and Libri Mundi \"Libri Mundi\". Arising from this, these were the reviewers that in my opinion were the most helpful ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<h4><li>Have reviews been getting more or less helpful over time?</li></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert reviewTime into datetime.\n",
    "\n",
    "df['reviewTime'] = pd.to_datetime(df['reviewTime'], format=\"%m %d, %Y\")\n",
    "\n",
    "#Create another dataframe containing only the necessary columns\n",
    "\n",
    "df_question3 = df[[\"reviewTime\", \"Helpfulness ratio\"]]\n",
    "\n",
    "#Sort by reviewTime to make it ascending.\n",
    "\n",
    "df_question3_ascending = df_question3.sort_values(by='reviewTime')\n",
    "df_question3_ascending.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating by Years\n",
    "\n",
    "df_question3_ascending[\"Year\"] = df_question3_ascending[\"reviewTime\"].dt.year\n",
    "\n",
    "df_question3_ascending = df_question3_ascending.groupby(['Year']).mean().reset_index()\n",
    "\n",
    "df_question3_ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a linear regression for the Helpfulness ratio from 2006 to 2014.\n",
    "\n",
    "# values converts it into a numpy array\n",
    "\n",
    "X = df_question3_ascending[\"Year\"].values.reshape(-1, 1)\n",
    "Y = df_question3_ascending[\"Helpfulness ratio\"].values.reshape(-1, 1)\n",
    "\n",
    "# create object for the class\n",
    "\n",
    "linear_regressor = LinearRegression() \n",
    "\n",
    "#Fit our model -> essentially generates a function that explains Y through X\n",
    "\n",
    "linear_regressor.fit(X, Y) \n",
    "\n",
    "# make predictions\n",
    "\n",
    "Y_pred = linear_regressor.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the two graphs\n",
    "\n",
    "plt.scatter(df_question3_ascending[\"Year\"], df_question3_ascending[\"Helpfulness ratio\"] , label= \"Ratio\", color= \"xkcd:navy\", linewidth=0.2) \n",
    "\n",
    "plt.plot(X, Y_pred, color='red')\n",
    "\n",
    "# X-axis & Y-axis label \n",
    "\n",
    "plt.xlabel('Year') \n",
    "plt.ylabel('Ratio') \n",
    "\n",
    "# Title \n",
    "\n",
    "plt.title('Review Helpfulness over time')\n",
    "\n",
    "# showing legend \n",
    "\n",
    "plt.legend() \n",
    "  \n",
    "# function to show the plot \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><li>Suggested Answer:</li></h4>\n",
    "From the question, I decided to aggregate the reviews by year, so that the analysis gets more efficient. Through the computations on the lines above, and through the display of the graph, It is clear to see that the helpfulness of the reviews have suffered a downfall. From 2006, the ratio was nearly 0.85, but at 2014, this value decreased substantially to approx. 0.15. The fall has been almost linear, which is visible with the comparation with the linear regression that was done, exactly to check how the reviews were reacting over the time. Although, this ratio has been decreasing over the year, this doesn't mean, we are having lower helpful reviews! On 2006, only three records were made, that explains the huge level of helpfulness, while on 2014, 5362 records were already made and the data about that year hadn't finish yet. This reveals that, despite the decrease in the ratio, part of it, can be explained through the higher increase of the total reviews being done, when compared to the helpful ones! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><li>After someone writes a review, will it be considered helpful by other\n",
    "users? Predictive Model with a Binary approach (Classification)</li></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to consider the dependent variable, the binary variable, by \"1\" if it is helpful or \"0\" otherwise. So the objective is to build a mathematical expression that will compute and predict whether a future review will be helpful based on past experiences. Secondly, I will describe a review by being helpful (\"1\") if it has an Helpfullness ratio higher or equal than 60%, and below this threshhold, I will consider it not helpful (\"0\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another columns associated with the past of the ID: Quality of review might be influenced whether the user already have done some reviews in the past.\n",
    "\n",
    "df[\"Nº Previous Total Reviews\"] = \"\"\n",
    "df[\"Nº Previous Helpful Reviews\"] = \"\"\n",
    "\n",
    "reviewID_list = df[\"reviewerID\"].tolist()\n",
    "uniqueUSER = list(sorted(set(reviewID_list)))\n",
    "\n",
    "#Empty Dataframe to fill after the loop.\n",
    "\n",
    "newDF = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in uniqueUSER:\n",
    "    df1 = df[df[\"reviewerID\"] == i]\n",
    "    df1.sort_values(by='reviewTime')\n",
    "\n",
    "    dfSIZE = np.arange(df1.shape[0])\n",
    "\n",
    "    number = 0\n",
    "    \n",
    "    #Loop for the Total reviews before the current one.\n",
    "\n",
    "    for e in dfSIZE:\n",
    "\n",
    "        df1.iloc[e, df1.columns.get_loc('Nº Previous Total Reviews')] = number\n",
    "        number = number + 1\n",
    "\n",
    "    number1 = 0\n",
    "    \n",
    "    #Loop for the Previous Helpful reviews before the currrent one.\n",
    "\n",
    "    for ii in dfSIZE:\n",
    "\n",
    "\n",
    "        if df1[\"Helpfulness ratio\"].iloc[ii] > 0.6:\n",
    "            df1.iloc[ii, df1.columns.get_loc('Nº Previous Helpful Reviews')] = number1\n",
    "            number1 = number1 + 1\n",
    "        else:\n",
    "            df1.iloc[ii, df1.columns.get_loc('Nº Previous Helpful Reviews')] = number1\n",
    "            number1 = number1 + 0\n",
    "            \n",
    "    #This gives us a certain Dataframe, df1, which contains the data for one ID, we just need to merge this DF with the empty one.\n",
    "    \n",
    "    newDF = newDF.append(df1)\n",
    "\n",
    "# newDF contains the Dataframe that contains the data regarding the past reviews.\n",
    "#newDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDFF = newDF[[\"reviewerID\", \"Nº Previous Helpful Reviews\", \"Nº Previous Total Reviews\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aggregated_df = pd.merge(df, newDFF, on='reviewerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment Analysis through textBlob\n",
    "\n",
    "Aggregated_df[\"reviewText\"] = Aggregated_df[\"reviewText\"].astype(str)\n",
    "Aggregated_df['Sentiment'] = Aggregated_df['summary'].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)\n",
    "\n",
    "#Convert Helpfulness ratio into binary variable: >0.6 => 1 ; <0.6 => 0\n",
    "\n",
    "Aggregated_df.loc[Aggregated_df['Helpfulness ratio'] >= 0.6, 'Binary Helpfulness ratio'] = 1\n",
    "Aggregated_df.loc[Aggregated_df['Helpfulness ratio'] < 0.6, 'Binary Helpfulness ratio'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Data for Test and Training sets\n",
    "\n",
    "modelling_X = Aggregated_df[[\"overall\", \"Sentiment\", \"Nº Previous Helpful Reviews_y\", 'Nº Previous Total Reviews_y']]\n",
    "modelling_Y = Aggregated_df[\"Binary Helpfulness ratio\"]\n",
    "\n",
    "#Convert everything to int\n",
    "\n",
    "modelling_X[\"Sentiment\"] = modelling_X[\"Sentiment\"].astype(int)\n",
    "modelling_Y = modelling_Y.to_frame()\n",
    "modelling_Y[\"Binary Helpfulness ratio\"] = modelling_Y[\"Binary Helpfulness ratio\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Algorithm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Defining our test and train mechanisms\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelling_X, modelling_Y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Fit our model -> essentially generates a multilinear function based on train data \n",
    "\n",
    "model = BernoulliNB().fit(X_train, y_train) \n",
    "\n",
    "#Apply that function into the test dataset\n",
    "\n",
    "y_pred_BAYES = model.predict(X_test)\n",
    "\n",
    "#Evaluate the model\n",
    "\n",
    "print(\"Accuracy of Naive Bayes classifier on test set:\", model.score(X_test, y_test))\n",
    "print(\"The f1 score is:\", f1_score(y_test, y_pred_BAYES))\n",
    "\n",
    "print(\"\\nFor example, an individual that reviewed a product with overall rating of \\\n",
    "1, with a text sentiment of 0.5, and that already had 1 helpful review in the past of 1 in total, \\\n",
    "will most likely, according to my model, generate a prediction of:\", model.predict([[1, 0.5, 1, 1]]), \". \\\n",
    "Meaning that it will be helful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Defining our test and train mechanisms\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelling_X, modelling_Y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Fit our model -> essentially generates a multilinear function based on train data \n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#Apply that function into the test dataset\n",
    "\n",
    "y_pred_LOGREG = logreg.predict(X_test)\n",
    "\n",
    "#Evaluate the model\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set:', logreg.score(X_test, y_test))\n",
    "print(\"The f1 score is:\", f1_score(y_test, y_pred_LOGREG))\n",
    "\n",
    "print(\"\\nFor exameple, an individual that reviewed a product with overall rating of \\\n",
    "5, with a text sentiment of 1, and that already had 1 helpful review in the past of 2 in total, \\\n",
    "will most likely, according to my model, generate a prediction of:\", logreg.predict([[5, 1,1, 2]]), \". \\\n",
    "Meaning that it will not helpful to others!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><li>Suggested Answer:</li></h4>\n",
    "In this question, I've built a model that aims to explain whether a future review will be helpful or not. It is a binary model, where the dependent variable is assigned with value 1 if it helpful, or value 0, otherwise. As independent variables, I selected the following variables: \"overall\", \"Sentiment\", \"Nº Previous Helpful Reviews_y\", \"Nº Previous Total Reviews_y\". I believe that the overall rating of the product will have a greater influence on the review, specially on low rating items, where customers will seek more information on these products, and so they will resort to these reviews. Following next, \"Sentiment\" reflects whether the text written by the user is, in fact, positive or negative regarding that specific product, my intuition followed the logic that if a reviewer describes the situation perfectly, than the sentiment analysis would provide an index extremely close to 1 or -1, and therefore would be more informative to the end customer, meaning that it could have an higher probability of being helpful. The last two variables are associated with the past of the reviewer, I believe that customers don't want fake reviews, and eventually will check if this reviewerID is definitely legit and has reputation, therefore the past of the user can play a major role in deciding whether the review will be helpful or not.\n",
    "\n",
    "From the selected variables, I conducted some train and test scenarios, with different classfiers: Naive Bayes and Logistic Regression. Why Have I chosen this two? So, the Naive Bayes, because it is a probabilistic model, and made sense to use it, since it's an algorithm that can be coded up easily and the predictions made real quick. Next, we have Logistic Regression, which works, exactly, when the dependent variable is binary.  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, ratio-level or other independent variables.\n",
    "\n",
    "After splitting our dataframe into test and train sets, I fit the train sets and this generated the functions that describes the relationship between the dependent and independent variables. From the Naive Bayes, I got an accuracy of 0.774, while with Logistic regression, this yield a value of 0.777, showing that this is a better approach.\n",
    "\n",
    "Some ideas to explore: I would try to use TPOT library to achieve the best accuracy level. Definitely lacks creativity regarding time, for example understanding the period when the review was made, and perhaps understand if this has any impact whether the review will be helpful or not (e.g. in Christmas time perhaps the probability of a review being helpful may increase). Plus, the sentiment analysis regarding the text that is on the reviews, in my opinion, requires a better text-processor in order to achieve higher results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bonus Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><li>Would you use it to build a new model to\n",
    "predict the helpfulness of a review?</li></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely yes! I wasn't familiar with this type of architecture, but from what I read It's encoder it's bidirectional, therefore it takes into account all the words in its surroundings, and not from only one direction, making this mechanism a better way to do sentiment analysis, which will most likely yield a higher accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><li>How do you expect that this new model’s performance compare with the previous one\n",
    "(from Exercise 2)? What makes the BERT model better/worse?</li></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expect that this new model behaves in a better way than the previous one, simply because it is the most complex and complete architecture at the moment, and that can amplify the results in a wide variety of NLP tasks. From \"Towards Data Science\", I've managed to understand that BERT reads the entire sequence of words at once. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word), which dethrones the idea of directional models, which read the text input sequentially (left-to-right or right-to-left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
